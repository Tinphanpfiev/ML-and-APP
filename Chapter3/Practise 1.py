# -*- coding: utf-8 -*-
"""Linear_regression.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1JYxCjtYgTBtPDG0l3_MpAUrTRzoIfj5l
"""

import numpy as np
import matplotlib.pyplot as plt

from re import A
# Data
x = np.array([155, 180, 164, 162, 181, 182, 173, 190, 171, 170, 181, 182, 189, 184, 209, 210])
y = np.array([51, 52, 54, 53, 55, 59, 61, 59, 63, 76, 64, 66, 69, 72, 70, 80])

# Initialize params
a = 0
b = 0
c = 1e-6
d = 1000

# Predict function
def h(x, a, b):
    return a + b * x

# Cost function
def cost_function(x, y, a, b):
    m = len(x)
    return (1/(2*m)) * np.sum((h(x, a, b) - y)**2)

# Gradient descent
def gradient_descent(x, y, a, b, c, d):
    m = len(x)
    cost_history = []
    for i in range(iterations):
        grad0 = (1/m) * np.sum(h(x, a, b) - y)
        grad1 = (1/m) * np.sum((h(x, a, b) - y) * x)
        a = a - alpha * grad0
        b = b - alpha * grad1
        cost = cost_function(x, y, a, b)
        cost_history.append(cost)
    return a, b, cost_history

def plot_final_result(x, y, a, b, cost, d):
    plt.close('all')
    fig, ax = plt.subplots()
    plt.xlabel('Time')
    plt.ylabel('Score')
    # Plot the original data points
    ax.scatter(x, y, color="blue", label="Data")

    # Plot the final prediction line based on theta0 and theta1
    line = a + b * x
    ax.plot(x, line, color="red", label="Prediction")
    ax.legend()

    ax.text(0.3, 0.95, f"Iteration: {iterations}, Cost: {cost:.4f}", transform=ax.transAxes)
    ax.text(0.3, 0.90, f"Theta0: {theta0:.4f}, Theta1: {theta1:.4f}", transform=ax.transAxes)

    plt.show()

a, b, cost_history = gradient_descent(x, y, a, b, c, d)
plot_final_result(x, y, a, b,  cost_history[-1], iterations)

